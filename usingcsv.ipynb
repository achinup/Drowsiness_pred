{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: dlib in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (19.24.99)\n",
      "Requirement already satisfied: pandas in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opencv-python) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python dlib pandas scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isfile(\"shape_predictor_68_face_landmarks.dat\"):\n",
    "    print(\"Error: Model file not found. Download it from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "\n",
    "# Load the models\n",
    "dnn_model = \"opencv_face_detector_uint8.pb\"\n",
    "dnn_proto = \"opencv_face_detector.pbtxt\"\n",
    "net = cv2.dnn.readNetFromTensorflow(dnn_model, dnn_proto)\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# EAR and MAR calculation functions\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    A = distance.euclidean(mouth[2], mouth[6])\n",
    "    B = distance.euclidean(mouth[3], mouth[5])\n",
    "    C = distance.euclidean(mouth[0], mouth[4])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "def head_tilt_angle(nose, chin):\n",
    "    dx = chin[0] - nose[0]\n",
    "    dy = chin[1] - nose[1]\n",
    "    return np.degrees(np.arctan2(dy, dx))\n",
    "\n",
    "# Improved face detection function using DNN\n",
    "def detect_faces_dnn(image):\n",
    "    h, w = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    faces = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:   # Confidence threshold\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x, y, x1, y1) = box.astype(\"int\")\n",
    "            faces.append((x, y, x1 - x, y1 - y))\n",
    "\n",
    "    return faces\n",
    "\n",
    "# Function to extract features from images\n",
    "def extract_features(image_path, label):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    image = cv2.resize(image, (400, 400))  # Resize for consistency\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = detect_faces_dnn(image)  # Use DNN for face detection\n",
    "\n",
    "    features = []\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        print(f\"No face detected in {image_path}\")\n",
    "        return None\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        rect = dlib.rectangle(int(x), int(y), int(x + w), int(y + h))\n",
    "        \n",
    "        # Landmark prediction\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = np.array([(shape.part(i).x, shape.part(i).y) for i in range(68)])\n",
    "\n",
    "        # Extract facial landmarks\n",
    "        left_eye = shape[36:42]\n",
    "        right_eye = shape[42:48]\n",
    "        mouth = shape[60:68]\n",
    "        nose = shape[27]\n",
    "        chin = shape[8]\n",
    "\n",
    "        ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2.0\n",
    "        mar = mouth_aspect_ratio(mouth)\n",
    "        tilt = head_tilt_angle(nose, chin)\n",
    "\n",
    "        features.append([ear, mar, tilt, label])\n",
    "\n",
    "    return features\n",
    "\n",
    "# Directory containing images\n",
    "base_dir = \"train\"\n",
    "labels = [\"awake_images\", \"drowsy_images\"]\n",
    "data = []\n",
    "\n",
    "# Extract features from all images\n",
    "for label in labels:\n",
    "    label_path = os.path.join(base_dir, label)\n",
    "    \n",
    "    for img_name in os.listdir(label_path):\n",
    "        img_path = os.path.join(label_path, img_name)\n",
    "        \n",
    "        try:\n",
    "            features = extract_features(img_path, label)\n",
    "            \n",
    "            if features:\n",
    "                data.extend(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_name}: {e}\")\n",
    "\n",
    "# Save features to CSV\n",
    "df = pd.DataFrame(data, columns=[\"EAR\", \"MAR\", \"Tilt\", \"Label\"])\n",
    "df.to_csv(\"facial_features.csv\", index=False)\n",
    "print(\"Feature extraction complete and saved to facial_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.5567 - loss: 0.7419 - val_accuracy: 0.5668 - val_loss: 0.6934\n",
      "Epoch 2/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5914 - loss: 0.7126 - val_accuracy: 0.5989 - val_loss: 0.6811\n",
      "Epoch 3/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6392 - loss: 0.6746 - val_accuracy: 0.6096 - val_loss: 0.6788\n",
      "Epoch 4/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6461 - loss: 0.6700 - val_accuracy: 0.6417 - val_loss: 0.6698\n",
      "Epoch 5/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6245 - loss: 0.6637 - val_accuracy: 0.6417 - val_loss: 0.6699\n",
      "Epoch 6/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6296 - loss: 0.6777 - val_accuracy: 0.6310 - val_loss: 0.6664\n",
      "Epoch 7/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6284 - loss: 0.6696 - val_accuracy: 0.6417 - val_loss: 0.6661\n",
      "Epoch 8/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6192 - loss: 0.6586 - val_accuracy: 0.6524 - val_loss: 0.6666\n",
      "Epoch 9/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6329 - loss: 0.6744 - val_accuracy: 0.6203 - val_loss: 0.6602\n",
      "Epoch 10/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6312 - loss: 0.6533 - val_accuracy: 0.6364 - val_loss: 0.6624\n",
      "Epoch 11/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6413 - loss: 0.6363 - val_accuracy: 0.6203 - val_loss: 0.6549\n",
      "Epoch 12/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6437 - loss: 0.6478 - val_accuracy: 0.6257 - val_loss: 0.6537\n",
      "Epoch 13/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6346 - loss: 0.6364 - val_accuracy: 0.6364 - val_loss: 0.6504\n",
      "Epoch 14/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6504 - loss: 0.6347 - val_accuracy: 0.6417 - val_loss: 0.6491\n",
      "Epoch 15/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6568 - loss: 0.6304 - val_accuracy: 0.6578 - val_loss: 0.6472\n",
      "Epoch 16/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6445 - loss: 0.6377 - val_accuracy: 0.6631 - val_loss: 0.6434\n",
      "Epoch 17/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6453 - loss: 0.6361 - val_accuracy: 0.6364 - val_loss: 0.6437\n",
      "Epoch 18/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6252 - loss: 0.6371 - val_accuracy: 0.6631 - val_loss: 0.6449\n",
      "Epoch 19/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6888 - loss: 0.6018 - val_accuracy: 0.6631 - val_loss: 0.6401\n",
      "Epoch 20/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6298 - loss: 0.6414 - val_accuracy: 0.6631 - val_loss: 0.6386\n",
      "Epoch 21/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6558 - loss: 0.6217 - val_accuracy: 0.6684 - val_loss: 0.6356\n",
      "Epoch 22/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6266 - loss: 0.6369 - val_accuracy: 0.6684 - val_loss: 0.6330\n",
      "Epoch 23/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6374 - loss: 0.6184 - val_accuracy: 0.6738 - val_loss: 0.6309\n",
      "Epoch 24/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6386 - loss: 0.6315 - val_accuracy: 0.6684 - val_loss: 0.6304\n",
      "Epoch 25/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6450 - loss: 0.6223 - val_accuracy: 0.6631 - val_loss: 0.6326\n",
      "Epoch 26/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6496 - loss: 0.6318 - val_accuracy: 0.6524 - val_loss: 0.6343\n",
      "Epoch 27/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6341 - loss: 0.6320 - val_accuracy: 0.6578 - val_loss: 0.6323\n",
      "Epoch 28/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6575 - loss: 0.6322 - val_accuracy: 0.6684 - val_loss: 0.6329\n",
      "Epoch 29/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6425 - loss: 0.6339 - val_accuracy: 0.6578 - val_loss: 0.6351\n",
      "Epoch 30/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6710 - loss: 0.6254 - val_accuracy: 0.6684 - val_loss: 0.6347\n",
      "Epoch 31/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6673 - loss: 0.6253 - val_accuracy: 0.6578 - val_loss: 0.6348\n",
      "Epoch 32/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6822 - loss: 0.6160 - val_accuracy: 0.6631 - val_loss: 0.6354\n",
      "Epoch 33/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6438 - loss: 0.6263 - val_accuracy: 0.6684 - val_loss: 0.6320\n",
      "Epoch 34/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6665 - loss: 0.6193 - val_accuracy: 0.6791 - val_loss: 0.6308\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6853 - loss: 0.6224 \n",
      "\n",
      "‚úÖ Test Accuracy: 0.6684492230415344\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.91      0.77       117\n",
      "           1       0.63      0.27      0.38        70\n",
      "\n",
      "    accuracy                           0.67       187\n",
      "   macro avg       0.65      0.59      0.58       187\n",
      "weighted avg       0.66      0.67      0.63       187\n",
      "\n",
      "\n",
      "üîé Confusion Matrix:\n",
      " [[106  11]\n",
      " [ 51  19]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# ---------------------------\n",
    "# üöÄ Load and preprocess the dataset\n",
    "# ---------------------------\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"facial_features.csv\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = df[[\"EAR\", \"MAR\", \"Tilt\"]].values\n",
    "y = df[\"Label\"].map({'awake_images': 0, 'drowsy_images': 1}).values  # Map labels\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape into 3D tensor -> (samples, height, width, channels)\n",
    "X_reshaped = X.reshape(-1, 3, 1, 1)\n",
    "\n",
    "# One-hot encode labels for CNN\n",
    "y_cnn = to_categorical(y)\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_cnn, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------------\n",
    "# ‚öôÔ∏è Define CNN model\n",
    "# ---------------------------\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Conv2D(32, kernel_size=(2, 1), activation='relu', input_shape=(3, 1, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Conv2D(64, kernel_size=(2, 1), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))  # 2 classes: awake, drowsy\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ---------------------------\n",
    "# üî• Train the CNN\n",
    "# ---------------------------\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# ---------------------------\n",
    "# üìä Evaluate the model\n",
    "# ---------------------------\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"\\n‚úÖ Test Accuracy:\", accuracy)\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nüìä Classification Report:\\n\", classification_report(y_true, y_pred_classes))\n",
    "print(\"\\nüîé Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6853 - loss: 0.6224 \n",
      "\n",
      "‚úÖ Test Accuracy: 0.6303648352622986\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"\\n‚úÖ Test Accuracy:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
